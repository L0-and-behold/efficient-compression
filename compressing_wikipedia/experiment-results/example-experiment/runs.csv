run_id,experiment_name,timestamp,run_time,alpha,pmmp,initial_p_value,beta,transformer_config,training_method,train_only_on_leading_tokens,epochs_prelude,epochs,epochs_fine_tuning,stop_epoch_at_batch_prelude,stop_epoch_at_batch,stop_epoch_at_batch_fine_tuning,batch_size,do_pruning,first_pruning_after,prune_every,use_pretrained_model,use_model_from_experiment,use_model_from_run,elapsed_epochs,learning_rate,seed,tolerated_relative_loss_increase,steps_per_chunk,log_every,checkpoint_time,max_runtime,seq_length,final_train_loss,final_val_loss,mean_train_loss,mean_test_loss,model_byte_size,non_zero_params,online_description_length_bytes,training_runtime,epsilon
run-9zav,example-experiment,2025-05-07 15:36:55,80.56687021255493,0.0001,True,0.7,10.0,transformer200k,<function drr_procedure at 0x7f059f996b60>,16384,1,1,1,False,False,False,4,True,1,1,False,,,1,1e-05,858,0.1,1,1,80000,86000,2048,5.545168399810791,,5.545168399810791,,2048,512,,43.23294758796692,0.7999999046325684
